{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attentive Music\n",
    "\n",
    "I plan to use a Transformer architecture to generate musical MIDI sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from music21 import *\n",
    "import os, sys\n",
    "import numpy as np\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "from torchsample.modules import ModuleTrainer\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "device = torch.device(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "I've found a [dataset](https://github.com/jukedeck/nottingham-dataset) of MIDI files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['waltzes7.mid',\n",
       " 'reelsa-c79.mid',\n",
       " 'reelsr-t57.mid',\n",
       " 'jigs211.mid',\n",
       " 'morris29.mid',\n",
       " 'reelsu-z8.mid',\n",
       " 'jigs156.mid',\n",
       " 'ashover5.mid',\n",
       " 'reelsa-c32.mid',\n",
       " 'morris10.mid']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PATH=\"../nottingham-dataset/MIDI\"\n",
    "files = [f for f in os.listdir(PATH) if os.path.isfile(PATH+'/'+f)]\n",
    "files[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From [this](https://www.hackerearth.com/blog/machine-learning/jazz-music-using-deep-learning/) tutorial for parsing MIDI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_notes(file_list, PATH):  \n",
    "    notes = []  \n",
    "    for file in tqdm(file_list):  \n",
    "    # converting .mid file to stream object\n",
    "        midi = converter.parse(PATH + '/' + file)  \n",
    "        notes_to_parse = [] \n",
    "        try:  \n",
    "            # Given a single stream, partition into a part for each unique instrument  \n",
    "            parts = instrument.partitionByInstrument(midi)  \n",
    "        except:  \n",
    "            pass  \n",
    "        if parts: # if parts has instrument parts   \n",
    "            notes_to_parse = parts.parts[0].recurse()  \n",
    "        else:  \n",
    "            notes_to_parse = midi.flat.notes  \n",
    "        for element in notes_to_parse:   \n",
    "            if isinstance(element, note.Note):  \n",
    "                # if element is a note, extract pitch   \n",
    "                notes.append(str(element.pitch))  \n",
    "            elif(isinstance(element, chord.Chord)):  \n",
    "                # if element is a chord, append the normal form of the   \n",
    "                # chord (a list of integers) to the list of notes.   \n",
    "                notes.append('.'.join(str(n) for n in element.normalOrder)) \n",
    "    \n",
    "    with open('data/notes', 'wb') as filepath:  \n",
    "        pickle.dump(notes, filepath)  \n",
    "    return notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create notes again\n",
    "# notes = get_notes(files, PATH)\n",
    "\n",
    "# Load from previously saved version\n",
    "if os.path.getsize('data/notes') > 0:\n",
    "    with open('data/notes', 'rb') as f:\n",
    "        unpickler = pickle.Unpickler(f)\n",
    "        notes = unpickler.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pitchnames = sorted(set(item for item in notes))\n",
    "note_to_int = dict((note, number) for number, note in enumerate(pitchnames))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[88, 111, 34, 108, 103, 88, 34, 110, 88, 94]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int_notes = [note_to_int[x] for x in notes]; int_notes[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = np.array([np.array(int_notes[i*bs:(i+1)*bs]) for i in range(len(int_notes)//bs)])\n",
    "ys = np.array([int_notes[(i+1)*bs] for i in range(len(int_notes)//bs)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 88, 111,  34, 108, 103,  88,  34, 110],\n",
       "       [ 88,  94,  67, 118,  94,  88,  34, 110],\n",
       "       [ 88, 111,  34, 108, 103,  88,  34, 110],\n",
       "       [ 88,  94,  44, 108,  97,  83, 103,  34],\n",
       "       [ 88, 111,  34, 108, 103,  88,  34, 110],\n",
       "       [ 88,  94,  67, 118,  94,  88,  34, 110],\n",
       "       [ 88, 111,  34, 108, 103,  88,  34, 110],\n",
       "       [ 88,  94,  44, 108,  97,  83, 103,  34],\n",
       "       [ 88, 111,  34, 108, 103,  88,  34, 110],\n",
       "       [ 88,  94,  67, 118,  94,  88,  34, 110]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xs[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the next notes in the sequence for each sequence in `xs`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([88, 88, 88, 88, 88, 88, 88, 88, 88, 88])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ys[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But our y data will need to be one-hot encoded for our training to work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(batch,vocab_size):\n",
    "    ones = torch.eye(vocab_size)\n",
    "    return ones.index_select(0,batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30727, 8)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tr, x_val, y_tr, y_val = train_test_split(xs[:30720], ys[:30720], test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tensor(from_int):\n",
    "    return torch.from_numpy(np.array(from_int)).long()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to create a class for our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MusicData(Dataset):\n",
    "\n",
    "    def __init__(self, x_data, y_data):\n",
    "        self.len = len(x_data)\n",
    "        self.x_data = tensor(x_data)\n",
    "        self.y_data = tensor(y_data)\n",
    "            \n",
    "    def __getitem__(self, index):\n",
    "        return self.x_data[index], one_hot(self.y_data[index],vocab_size=120).squeeze(0).long()\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_data = MusicData(x_tr, y_tr)\n",
    "val_data = MusicData(x_val, y_val)\n",
    "\n",
    "tr_loader = DataLoader(dataset=tr_data,\n",
    "                       batch_size=32,\n",
    "                       shuffle=True,\n",
    "                       num_workers=1,\n",
    "                       pin_memory=True)\n",
    "val_loader = DataLoader(dataset=val_data,\n",
    "                        batch_size=32,\n",
    "                        shuffle=True,\n",
    "                        num_workers=1,\n",
    "                        pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM\n",
    "\n",
    "Let's first try an LSTM as a simple example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMTagger(nn.Module):\n",
    "\n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size, num_layers, batch_size):\n",
    "        super(LSTMTagger, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.batch_size = batch_size\n",
    "        self.vocab_size = vocab_size\n",
    "\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_size, dropout=0.5)\n",
    "        self.hidden2tag = nn.Linear(hidden_dim, vocab_size)\n",
    "        self.hidden = self.init_hidden()\n",
    "\n",
    "    def init_hidden(self):\n",
    "        # Before we've done anything, we dont have any hidden state.\n",
    "        # Refer to the Pytorch documentation to see exactly\n",
    "        # why they have this dimensionality.\n",
    "        # The axes semantics are (num_layers, minibatch_size, hidden_dim)\n",
    "        return (torch.zeros(self.batch_size, self.num_layers, self.hidden_dim).cuda(),\n",
    "                torch.zeros(self.batch_size, self.num_layers, self.hidden_dim).cuda())\n",
    "\n",
    "    def forward(self, sentence):\n",
    "        embeds = self.word_embeddings(sentence)\n",
    "        lstm_out, self.hidden = self.lstm(\n",
    "            embeds.view(sentence.size(0),sentence.size(1), -1), self.hidden)\n",
    "        tag_space = self.hidden2tag(lstm_out.view(sentence.size(0),sentence.size(1), -1))\n",
    "#         tag_scores = F.log_softmax(tag_space, dim=-1).view(self.batch_size, -1, self.vocab_size)\n",
    "        tag_scores = F.log_softmax(tag_space, dim=-1)\n",
    "        return tag_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "379d1c4f6bc7445e8d233adea41fba11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=4, style=ProgressStyle(description_width='initial…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "856b381a28474ff9a865d84204a0f8bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Batch', max=720, style=ProgressStyle(description_width='initi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.0794, device='cuda:0', grad_fn=<NllLoss2DBackward>)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Process-40:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/jhbremner/.conda/envs/attentive-music/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/jhbremner/.conda/envs/attentive-music/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/jhbremner/.conda/envs/attentive-music/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 96, in _worker_loop\n",
      "    r = index_queue.get(timeout=MANAGER_STATUS_CHECK_INTERVAL)\n",
      "  File \"/home/jhbremner/.conda/envs/attentive-music/lib/python3.7/multiprocessing/queues.py\", line 104, in get\n",
      "    if not self._poll(timeout):\n",
      "  File \"/home/jhbremner/.conda/envs/attentive-music/lib/python3.7/multiprocessing/connection.py\", line 257, in poll\n",
      "    return self._poll(timeout)\n",
      "  File \"/home/jhbremner/.conda/envs/attentive-music/lib/python3.7/multiprocessing/connection.py\", line 414, in _poll\n",
      "    r = wait([self], timeout)\n",
      "  File \"/home/jhbremner/.conda/envs/attentive-music/lib/python3.7/multiprocessing/connection.py\", line 920, in wait\n",
      "    ready = selector.select(timeout)\n",
      "  File \"/home/jhbremner/.conda/envs/attentive-music/lib/python3.7/selectors.py\", line 415, in select\n",
      "    fd_event_list = self._selector.poll(timeout)\n",
      "KeyboardInterrupt\n",
      "Exception ignored in: <function _DataLoaderIter.__del__ at 0x7f35d364d400>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/jhbremner/.conda/envs/attentive-music/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 397, in __del__\n",
      "    def __del__(self):\n",
      "  File \"/home/jhbremner/.conda/envs/attentive-music/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 227, in handler\n",
      "    _error_if_any_worker_fails()\n",
      "RuntimeError: DataLoader worker (pid 3486) exited unexpectedly with exit code 1. Details are lost due to multiprocessing. Rerunning with num_workers=0 may give better error trace.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-107-def16a607fd2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;31m#  calling optimizer.step()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtag_scores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\r'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/attentive-music/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m     91\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \"\"\"\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/attentive-music/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = LSTMTagger(embedding_dim=50,hidden_dim=128,vocab_size=len(note_to_int), num_layers=8, batch_size=32).cuda()\n",
    "loss_function = nn.CrossEntropyLoss().cuda()\n",
    "optimizer = optim.SGD(model.parameters(), lr=1)\n",
    "\n",
    "for epoch in tqdm(range(4), desc='Epoch'):  # again, normally you would NOT do 4 epochs, it is toy data\n",
    "    for i, (inputs, labels) in enumerate(tqdm(tr_loader, desc='Batch')):\n",
    "        \n",
    "        # Step 1. Remember that Pytorch accumulates gradients.\n",
    "        # We need to clear them out before each instance\n",
    "        model.zero_grad()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Also, we need to clear out the hidden state of the LSTM,\n",
    "        # detaching it from its history on the last instance.\n",
    "        model.hidden = model.init_hidden()\n",
    "\n",
    "        # Step 2. Get our inputs ready for the network, that is, turn them into\n",
    "        # Tensors of word indices.\n",
    "        inputs, labels = Variable(inputs), Variable(labels)\n",
    "\n",
    "        # Step 3. Run our forward pass.\n",
    "        tag_scores = model(inputs.cuda())\n",
    "\n",
    "        # Step 4. Compute the loss, gradients, and update the parameters by\n",
    "        #  calling optimizer.step()\n",
    "        loss = loss_function(tag_scores.cuda(), labels.cuda())\n",
    "        loss.backward()\n",
    "        sys.stdout.write('\\r'+str(loss))\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'lstm_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTMTagger(embedding_dim=50,hidden_dim=128,vocab_size=len(note_to_int), num_layers=8, batch_size=32)\n",
    "model.load_state_dict(torch.load('lstm_model'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 94, 100,  81,  88,  88,  57, 103, 100],\n",
       "        [110,  88,  94,  67,  88, 103,  34, 108],\n",
       "        [ 94,  88,  81,  89, 119,  44, 108, 108],\n",
       "        [ 94, 110, 102, 107, 110, 115,  47,  94],\n",
       "        [102,  57, 118, 118, 110, 118,  67,  88],\n",
       "        [107,  47, 110, 107, 102,  96,  83,  93],\n",
       "        [ 94,  97,  83, 103, 108,  97, 103,  34],\n",
       "        [ 89, 111,  12, 103, 103,  34, 108, 103],\n",
       "        [100,  34, 108, 103, 100,  94,  67, 118],\n",
       "        [119, 111, 108, 111, 119, 103,  94,  67]])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(tr_loader))[0][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags = next(iter(tr_loader))[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 88, 111, 107, 111, 108, 103, 111, 102, 103,  97, 103, 102,  89,\n",
       "        88, 118, 103, 107, 102, 108, 103,  34, 103,  89, 118,  97, 119,\n",
       "        88,  91,  97, 100,  94, 108])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(tags.cpu().detach().numpy(), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-4.7930, -4.7547, -4.7560,  ..., -4.7498, -4.7386, -4.8113],\n",
       "         [-4.7958, -4.7464, -4.7515,  ..., -4.7518, -4.7369, -4.8163],\n",
       "         [-4.7985, -4.7524, -4.7524,  ..., -4.7527, -4.7391, -4.8094],\n",
       "         ...,\n",
       "         [-4.7953, -4.7445, -4.7464,  ..., -4.7536, -4.7385, -4.8071],\n",
       "         [-4.7961, -4.7466, -4.7508,  ..., -4.7602, -4.7323, -4.8092],\n",
       "         [-4.7944, -4.7486, -4.7491,  ..., -4.7545, -4.7389, -4.8050]],\n",
       "\n",
       "        [[-4.7887, -4.7514, -4.7542,  ..., -4.7551, -4.7361, -4.8152],\n",
       "         [-4.7948, -4.7476, -4.7497,  ..., -4.7550, -4.7355, -4.8122],\n",
       "         [-4.8025, -4.7467, -4.7532,  ..., -4.7530, -4.7386, -4.8017],\n",
       "         ...,\n",
       "         [-4.7903, -4.7481, -4.7435,  ..., -4.7522, -4.7374, -4.8046],\n",
       "         [-4.7904, -4.7485, -4.7486,  ..., -4.7542, -4.7427, -4.8121],\n",
       "         [-4.7942, -4.7501, -4.7520,  ..., -4.7568, -4.7390, -4.8053]],\n",
       "\n",
       "        [[-4.7927, -4.7517, -4.7482,  ..., -4.7545, -4.7369, -4.8143],\n",
       "         [-4.7891, -4.7529, -4.7540,  ..., -4.7504, -4.7324, -4.8083],\n",
       "         [-4.7987, -4.7474, -4.7559,  ..., -4.7508, -4.7414, -4.8086],\n",
       "         ...,\n",
       "         [-4.7892, -4.7455, -4.7471,  ..., -4.7563, -4.7409, -4.8099],\n",
       "         [-4.7926, -4.7521, -4.7511,  ..., -4.7542, -4.7446, -4.8156],\n",
       "         [-4.7943, -4.7495, -4.7582,  ..., -4.7594, -4.7372, -4.8084]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-4.7889, -4.7487, -4.7596,  ..., -4.7457, -4.7368, -4.8081],\n",
       "         [-4.7941, -4.7543, -4.7478,  ..., -4.7546, -4.7395, -4.8086],\n",
       "         [-4.7893, -4.7529, -4.7489,  ..., -4.7505, -4.7423, -4.8130],\n",
       "         ...,\n",
       "         [-4.7977, -4.7495, -4.7488,  ..., -4.7507, -4.7327, -4.8098],\n",
       "         [-4.7905, -4.7496, -4.7521,  ..., -4.7522, -4.7402, -4.8111],\n",
       "         [-4.7927, -4.7512, -4.7504,  ..., -4.7591, -4.7403, -4.8101]],\n",
       "\n",
       "        [[-4.7888, -4.7539, -4.7559,  ..., -4.7494, -4.7359, -4.8091],\n",
       "         [-4.7893, -4.7512, -4.7524,  ..., -4.7518, -4.7395, -4.8059],\n",
       "         [-4.7918, -4.7533, -4.7424,  ..., -4.7566, -4.7430, -4.8148],\n",
       "         ...,\n",
       "         [-4.7935, -4.7508, -4.7470,  ..., -4.7564, -4.7368, -4.8116],\n",
       "         [-4.7897, -4.7474, -4.7524,  ..., -4.7597, -4.7388, -4.8091],\n",
       "         [-4.7955, -4.7524, -4.7523,  ..., -4.7497, -4.7351, -4.8107]],\n",
       "\n",
       "        [[-4.7887, -4.7473, -4.7490,  ..., -4.7562, -4.7354, -4.8097],\n",
       "         [-4.7986, -4.7494, -4.7535,  ..., -4.7473, -4.7389, -4.8126],\n",
       "         [-4.7904, -4.7533, -4.7492,  ..., -4.7546, -4.7403, -4.8172],\n",
       "         ...,\n",
       "         [-4.7928, -4.7503, -4.7515,  ..., -4.7532, -4.7443, -4.8123],\n",
       "         [-4.7931, -4.7481, -4.7510,  ..., -4.7580, -4.7405, -4.8053],\n",
       "         [-4.7936, -4.7482, -4.7495,  ..., -4.7532, -4.7361, -4.8115]]],\n",
       "       device='cuda:0', grad_fn=<LogSoftmaxBackward>)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(next(iter(tr_loader))[0].cuda()).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model(next(iter(tr_loader))[0].cuda()).cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 8, 120)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[88, 88, 88, 88, 88, 88, 88, 88],\n",
       "       [88, 88, 88, 88, 88, 88, 88, 88],\n",
       "       [88, 88, 88, 88, 88, 88, 88, 88],\n",
       "       [88, 88, 88, 88, 88, 88, 88, 88],\n",
       "       [88, 88, 88, 88, 88, 88, 88, 88],\n",
       "       [88, 88, 88, 88, 78, 88, 88, 88],\n",
       "       [88, 88, 88, 88, 78, 88, 88, 88],\n",
       "       [88, 88, 88, 88, 88, 88, 88, 88],\n",
       "       [88, 88, 88, 88, 88, 88, 88, 88],\n",
       "       [88, 88, 88, 88, 88, 88, 88, 88],\n",
       "       [88, 88, 88, 88, 88, 88, 88, 88],\n",
       "       [88, 88, 88, 88, 88, 88, 88, 88],\n",
       "       [88, 88, 88, 88, 88, 88, 88, 88],\n",
       "       [88, 88, 88, 88, 88, 88, 88, 88],\n",
       "       [88, 88, 88, 88, 88, 88, 88, 88],\n",
       "       [88, 88, 88, 88, 88, 88, 88, 88],\n",
       "       [88, 88, 88, 88, 88, 88, 88, 88],\n",
       "       [88, 88, 88, 88, 88, 88, 88, 88],\n",
       "       [88, 88, 88, 88, 88, 88, 88, 88],\n",
       "       [88, 88, 88, 88, 88, 88, 88, 88],\n",
       "       [88, 88, 88, 88, 88, 88, 88, 88],\n",
       "       [88, 88, 88, 88, 88, 88, 88, 88],\n",
       "       [88, 88, 88, 88, 88, 88, 88, 88],\n",
       "       [88, 88, 42, 88, 88, 88, 88, 88],\n",
       "       [88, 88, 88, 88, 88, 88, 88, 88],\n",
       "       [88, 88, 88, 88, 88, 88, 88, 88],\n",
       "       [88, 88, 88, 88, 88, 88, 88, 88],\n",
       "       [88, 88, 88, 88, 88, 88, 88, 88],\n",
       "       [88, 88, 88, 88, 88, 88, 88, 88],\n",
       "       [88, 88, 88, 88, 88, 88, 88, 88],\n",
       "       [88, 88, 88, 88, 88, 88, 88, 88],\n",
       "       [88, 88, 88, 88, 88, 88, 88, 88]])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(preds, axis=2)+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 120])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tags.size()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
